{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db554735",
   "metadata": {},
   "source": [
    "# Chaining and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8828cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"educate.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4041f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c53903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_16388\\4051320023.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "d:\\AIML\\LangChain-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae5d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_16388\\4179488048.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# load llama2 model\n",
    "llm=Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6a6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the context provided below.\n",
    "Think step by step and provide a detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6a223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain introduction\n",
    "# Create document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51064e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B429575D50>, search_kwargs={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Retrivers: An interface which connects to the vector db and returns documents given an unstructured query\n",
    "It does not need to be able to store the documents, just return them.\"\"\"\n",
    "\n",
    "retriver = db.as_retriever()\n",
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f17518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever Chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriver, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4336f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In simple terms, an activation function is a mathematical function that takes a input value and transforms it into an output value with a specific shape or pattern. The output value can be either a number (in the case of a continuous activation function) or a boolean value (in the case of a discrete activation function).\\n\\nThe main purpose of an activation function is to introduce non-linearity into a neural network, which allows it to learn and represent more complex relationships between inputs and outputs. Without activation functions, neural networks would only be able to learn linear relationships, which are not sufficient for solving many real-world problems.\\n\\nSome common activation functions used in deep learning include:\\n\\n1. Sigmoid: This function maps the input value to a output value between 0 and 1. It is often used in the hidden layers of a neural network, as it allows the network to learn linear or logistic relationships between the inputs and outputs.\\n2. ReLU (Rectified Linear Unit): This function maps all negative values to 0 and all positive values to the same value. It is widely used in deep learning because it is simple to compute and can help to prevent the vanishing gradient problem.\\n3. Tanh (Hyperbolic Tangent): This function maps the input value to a output value between -1 and 1. It is similar to the sigmoid function, but has a different shape and can be used to model different types of relationships.\\n4. Softmax: This function maps a vector of real values to a probability distribution over a set of classes. It is commonly used in the output layer of a neural network for classification tasks.\\n\\nIn summary, an activation function is a mathematical function that transforms an input value into an output value with a specific shape or pattern, and is used in deep learning to introduce non-linearity into a neural network and help it learn more complex relationships between inputs and outputs.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\" : \"What is an activation function? Explain in simple terms\"})['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
